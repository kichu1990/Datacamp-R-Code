# The most common first step when conducting time series analysis is to display your time series dataset in a visually intuitive format. 
# The most useful way to view raw time series data in R is to use the print() command, which displays the Start, End, and Frequency of 
# your data along with the observations.
# Print the Nile dataset
print(Nile)

# List the number of observations in the Nile dataset
length(Nile)

# Display the first 10 elements of the Nile dataset
head(Nile,n=10)

# Display the last 12 elements of the Nile dataset
tail(Nile,n=12)

# The plot() command is one of the most versatile commands in R. When used with time series data, this command automatically plots your 
# data over time.
# Plot the Nile data
plot(Nile)

# Plot the Nile data with xlab and ylab arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})")

# Plot the Nile data with xlab, ylab, main, and type arguments
plot(Nile, xlab = "Year", ylab = "River Volume (1e9 m^{3})",
main = "Annual River Nile Volume at Aswan, 1871-1970", type ="b")

# Plot the continuous_series using continuous time indexing
par(mfrow=c(2,1))
plot(continuous_time_index, continuous_series, type = "b")

# Make a discrete time index using 1:20 
discrete_time_index <- 1:20

# Now plot the continuous_series using discrete time indexing
plot(discrete_time_index, continuous_series, type = "b")

# The start() and end() functions return the time index of the first and last observations, respectively. 
# The time() function calculates a vector of time indices, with one element for each time index on which the series was observed.
# The deltat() function returns the fixed time interval between observations and the frequency() function returns the number of 
# observations per unit time. Finally, the cycle() function returns the position in the cycle of each observation.
# Plot AirPassengers
plot(AirPassengers)

# View the start and end dates of AirPassengers
start(AirPassengers)
end(AirPassengers)

# Use time(), deltat(), frequency(), and cycle() with AirPassengers 
time(AirPassengers)
deltat(AirPassengers)
frequency(AirPassengers)
cycle(AirPassengers)

# Sometimes there are missing values in time series data, denoted NA in R, and it is useful to know their locations. It is also 
# important to know how missing values are handled by various R functions. Sometimes we may want to ignore any missingness, but other 
# times we may wish to impute or estimate the missing values.
# Plot the AirPassengers data
plot(AirPassengers)

# Compute the mean of AirPassengers
mean(AirPassengers, na.rm = TRUE)

# Impute mean values to NA in AirPassengers
AirPassengers[85:96] <- mean(AirPassengers, na.rm = TRUE)

# Generate another plot of AirPassengers
plot(AirPassengers)

# Add the complete AirPassengers data to your plot
rm(AirPassengers)
points(AirPassengers, type = "l", col = 2, lty = 3)

# Use print() and plot() to view data_vector
print(data_vector)
plot(data_vector)

# Convert data_vector to a ts object with start = 2004 and frequency = 4
time_series <- ts(data_vector,start=2004,frequency=4)

# Use print() and plot() to view time_series
print(time_series)
plot(time_series)

# Check whether data_vector and time_series are ts objects
is.ts(data_vector)
is.ts(time_series)

# Check whether Nile is a ts object
is.ts(Nile)

# Check whether AirPassengers is a ts object
is.ts(AirPassengers)

# Check whether eu_stocks is a ts object
is.ts(eu_stocks)

# View the start, end, and frequency of eu_stocks
start(eu_stocks)
end(eu_stocks)
frequency(eu_stocks)

# Generate a simple plot of eu_stocks
plot(eu_stocks)

# Use ts.plot with eu_stocks
ts.plot(eu_stocks, col = 1:4, xlab = "Year", ylab = "Index Value", main = "Major European Stock Indices, 1991-1998")

# Add a legend to your ts.plot
legend("topleft", colnames(eu_stocks), lty = 1, col = 1:4, bty = "n")

# The logarithmic function log() is a data transformation that can be applied to positively valued time series data. It slightly 
# shrinks observations that are greater than one towards zero, while greatly shrinking very large observations. This property can 
# stabilize variability when a series exhibits increasing variability over time. It may also be used to linearize a rapid growth pattern
# over time.
# Log rapid_growth
linear_growth <- log(rapid_growth)
  
# Plot linear_growth using ts.plot()
ts.plot(linear_growth) 

# Generate the first difference of z
dz <- diff(z)
  
# Plot dz
ts.plot(dz)

# View the length of z and dz, respectively
length(z)
length(dz)

# For time series exhibiting seasonal trends, seasonal differencing can be applied to remove these periodic patterns. 
# For example, monthly data may exhibit a strong twelve month pattern. In such situations, changes in behavior from year to year may 
# be of more interest than changes from month to month, which may largely follow the overall seasonal pattern.

# When using the diff() command, the lag argument will specify the number of periods you are differencing by. In this exercise, you 
# should type diff(x, lag = 4) to generate a 4-period difference. If you're confused, you can access the help documentation by typing 
#?diff into your R console.
# Generate a diff of x with n = 4. Save this to dx
dx <- diff(x, lag = 4)
  
# Plot dx
ts.plot(dx)  

# View the length of x and dx, respectively 
length(x)
length(dx)

# The white noise (WN) model is a basic time series model. It is also a basis for the more elaborate models we will consider. 
# We will focus on the simplest form of WN, independent and identically distributed data.
# The arima.sim() function can be used to simulate data from a variety of time series models. 
# ARIMA is an abbreviation for the autoregressive integrated moving average class of models we will consider throughout this course.
# An ARIMA(p, d, q) model has three parts, the autoregressive order p, the order of integration (or differencing) d, and the 
# moving average order q. We will detail each of these parts soon, but for now we note that the ARIMA(0, 0, 0) model, i.e., 
# with all of these components zero, is simply the WN model.

# Simulate a WN model with list(order = c(0, 0, 0))
white_noise<- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

# Plot the WN observations
ts.plot(white_noise)

# Simulate from the WN model with: mean = 100, sd = 10
white_noise_2 <- arima.sim(list(order = c(0, 0, 0)), n = 100, mean = 100, sd = 10)

# Plot your white_noise_2 data
ts.plot(white_noise_2)

# Fit the WN model to y using the arima command
arima(y, order = c(0, 0, 0))

# Calculate the sample mean and sample variance of y
mean(y)
var(y)

# The random walk (RW) model is also a basic time series model. It is the cumulative sum (or integration) of a mean zero white noise
# (WN) series, such that the first difference series of a RW is a WN series. Note for reference that the RW model is an ARIMA(0, 1, 0)
# model, in which the middle entry of 1 indicates that the model's order of integration is 1.

# Generate a RW model using arima.sim
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)

# Plot random_walk
ts.plot(random_walk)

# Calculate the first difference series
random_walk_diff <- diff(random_walk)

# Plot random_walk_diff
ts.plot(random_walk_diff)

# Generate a RW model with a drift uing arima.sim
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1)

# Plot rw_drift
ts.plot(rw_drift)

# Calculate the first difference series
rw_drift_diff <- diff(rw_drift)

# Plot rw_drift_diff
ts.plot(rw_drift_diff)

# Difference your random_walk data
rw_diff <- diff(random_walk)

# Plot rw_diff
ts.plot(rw_diff)

# Now fit the WN model to the differenced data
model_wn <- arima(rw_diff, order = c(0, 0, 0))

# Copy and paste the value of the estimated time trend (intercept) below
int_wn <- model_wn$coef

# Plot the original random_walk data
ts.plot(random_walk)

# Use abline(0, ...) to add time trend to the figure
abline(0, int_wn)

# The white noise (WN) and random walk (RW) models are very closely related. However, only the RW is always non-stationary, both with 
# and without a drift term. This is a simulation exercise to highlight the differences.

# Use arima.sim() to generate WN data
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

# Use cumsum() to convert your WN data to RW
random_walk <- cumsum(white_noise)
  
# Use arima.sim() to generate WN drift data
wn_drift <- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 0.4)
  
# Use cumsum() to convert your WN drift data to RW
rw_drift <- cumsum(wn_drift)

# Plot all four data objects
plot.ts(cbind(white_noise, random_walk, wn_drift, rw_drift))

# The goal of investing is to make a profit. The revenue or loss from investing depends on the amount invested and changes in prices, 
# and high revenue relative to the size of an investment is of central interest. This is what financial asset returns measure, changes 
# in price as a fraction of the initial price over a given time horizon, for example, one business day.
# Log returns, also called continuously compounded returns, are also commonly used in financial time series analysis. 
# They are the log of gross returns, or equivalently, the changes (or first differences) in the logarithm of prices.
# The change in appearance between daily prices and daily returns is typically substantial, while the difference between daily returns 
# and log returns is usually small.

# Plot eu_stocks
plot(eu_stocks)

# Use this code to convert prices to returns
returns <- eu_stocks[-1,] / eu_stocks[-1860,] - 1

# Convert returns to ts
returns <- ts(returns, start = c(1991, 130), frequency = 260)

# Plot returns
plot(returns)

# Use this code to convert prices to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)

# Daily financial asset returns typically share many characteristics. Returns over one day are typically small, and their average is 
# close to zero. At the same time, their variances and standard deviations can be relatively large. Over the course of a few years, 
# several very large returns (in magnitude) are typically observed. These relative outliers happen on only a handful of days, but 
# they account for the most substantial movements in asset prices. Because of these extreme returns, the distribution of daily asset 
# returns is not normal, but heavy-tailed, and sometimes skewed. In general, individual stock returns typically have even greater 
# variability and more extreme observations than index returns.

# Generate means from eu_percentreturns
colMeans(eu_percentreturns)

# Use apply to calculate sample variance from eu_percentreturns
apply(eu_percentreturns, MARGIN = 2, FUN = var)

# Use apply to calculate standard deviation from eu_percentreturns
apply(eu_percentreturns, MARGIN = 2, FUN = sd)

# Display a histogram of percent returns for each index
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = hist, main = "", xlab = "Percentage Return")

# Display normal quantile plots of percent returns for each index
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = qqnorm, main = "")
qqline(eu_percentreturns)

# Make a scatterplot of DAX and FTSE
plot(DAX,FTSE)

# Make a scatterplot matrix of eu_stocks
pairs(eu_stocks)

# Convert eu_stocks to log returns
logreturns <- diff(log(eu_stocks))

# Plot logreturns
plot(logreturns)

# Make a scatterplot matrix of logreturns
pairs(logreturns)

# Sample covariances measure the strength of the linear relationship between matched pairs of variables. The cov() function can be used 
# to calculate covariances for a pair of variables, or a covariance matrix when a matrix containing several variables is given as input.
# For the latter case, the matrix is symmetric with covariances between variables on the off-diagonal and variances of the variables 
# along the diagonal. On the right you can see the scatterplot matrix of your logreturns data.
# Covariances are very important throughout finance, but they are not scale free and they can be difficult to directly interpret. 
# Correlation is the standardized version of covariance that ranges in value from -1 to 1, where values close to 1 in magnitude 
# indicate a strong linear relationship between pairs of variables. The cor() function can be applied to both pairs of variables as 
# well as a matrix containing several variables, and the output is interpreted analogously.

# Use cov() with DAX_logreturns and FTSE_logreturns
cov(DAX_logreturns,FTSE_logreturns)

# Use cov() with logreturns
cov(logreturns)

# Use cor() with DAX_logreturns and FTSE_logreturns
cor(DAX_logreturns,FTSE_logreturns)

# Use cor() with logreturns
cor(logreturns)

# Autocorrelations or lagged correlations are used to assess whether a time series is dependent on its past. For a time series x of 
# length n we consider the n-1 pairs of observations one time unit apart. The first such pair is (x[2],x[1]), and the next is 
# (x[3],x[2]). Each such pair is of the form (x[t],x[t-1]) where t is the observation index, which we vary from 2 to n in this case. 
# The lag-1 autocorrelation of x can be estimated as the sample correlation of these (x[t], x[t-1]) pairs.

# Define x_t0 as x[-1]
x_t0 <- x[-1] 

# Define x_t1 as x[-n]
x_t1 <- x[-n]

# Confirm that x_t0 and x_t1 are (x[t], x[t-1]) pairs  
head(cbind(x_t0, x_t1))
  
# Plot x_t0 and x_t1
plot(x_t0, x_t1)

# View the correlation between x_t0 and x_t1
cor(x_t0, x_t1)

# Use acf with x
acf(x, lag.max = 1, plot = FALSE)

# Confirm that difference factor is (n-1)/n 
cor(x_t1, x_t0) * (n-1)/n

# Autocorrelations can be estimated at many lags to better assess how a time series relates to its past. We are typically most 
# interested in how a series relates to its most recent past.
# Generate ACF estimates for x up to lag-10
acf(x, lag.max = 10, plot = FALSE)

# Type the ACF estimate at lag-10 
0.100

# Type the ACF estimate at lag-5
0.198

# Estimating the autocorrelation function (ACF) at many lags allows us to assess how a time series x relates to its past. The numeric 
# estimates are important for detailed calculations, but it is also useful to visualize the ACF as a function of the lag.
# In fact, the acf() command produces a figure by default. It also makes a default choice for lag.max, the maximum number of lags to be 
# displayed.
# View the ACF of x
acf(x)

# View the ACF of y
acf(y)

# View the ACF of z
acf(z)

# The autoregressive (AR) model is arguably the most widely used time series model. It shares the very familiar interpretation of a 
# simple linear regression, but here each observation is regressed on the previous observation. The AR model also includes the white 
# noise (WN) and random walk (RW) models 
# The versatile arima.sim() function used in previous chapters can also be used to simulate data from an AR model by setting the model 
# argument equal to list(ar = phi) , in which phi is a slope parameter from the interval (-1, 1). We also need to specify a series 
# length n.

# Simulate an AR model with 0.5 slope
x <- arima.sim(model = list(ar = 0.5), n = 100)

# Simulate an AR model with 0.9 slope
y <- arima.sim(model = list(ar = 0.9), n = 100)

# Simulate an AR model with -0.75 slope
z <- arima.sim(model = list(ar = -0.75), n = 100)

# Plot your simulated data
plot.ts(cbind(x, y,z))

# Calculate the ACF for x
acf(x)

# Calculate the ACF for y
acf(y)

# Calculate the ACF for z
acf(z)

# The random walk (RW) model is a special case of the autoregressive (AR) model, in which the slope parameter is equal to 1. Recall 
# from previous chapters that the RW model is not stationary and exhibits very strong persistence. Its sample autocovariance function 
# (ACF) also decays to zero very slowly, meaning past values have a long lasting impact on current values.
# The stationary AR model has a slope parameter between -1 and 1. The AR model exhibits higher persistence when its slope parameter is 
# closer to 1, but the process reverts to its mean fairly quickly. Its sample ACF also decays to zero at a quick (geometric) rate,
# indicating that values far in the past have little impact on future values of the process.

# Simulate and plot AR model with slope 0.9 
x <- arima.sim(model = list(ar = 0.9), n = 200)
ts.plot(x)
acf(x)

# Simulate and plot AR model with slope 0.98
y <- arima.sim(model = list(ar = 0.98), n = 200)
ts.plot(y)
acf(y)

# Simulate and plot RW model
z <- arima.sim(model = list(order=c(0,1,0)), n = 200)
ts.plot(z)
acf(z) 

# For a given time series x we can fit the autoregressive (AR) model using the arima() command and setting order equal to c(1, 0, 0). 
# Note for reference that an AR model is an ARIMA(1, 0, 0) model.

# Fit the AR model to x
arima(x, order = c(1, 0, 0))

# Copy and paste the slope (ar1) estimate
0.8575

# Copy and paste the slope mean (intercept) estimate
-0.0948

# Copy and paste the innovation variance (sigma^2) estimate
1.022

# Fit the AR model to AirPassengers
AR <- arima(AirPassengers, order = c(1, 0, 0))
print(AR)

# Run the following commands to plot the series and fitted values
ts.plot(AirPassengers)
AR_fitted <- AirPassengers - residuals(AR)
points(AR_fitted, type = "l", col = 2, lty = 2)

# Fit an AR model to Nile
AR_fit <- arima(Nile, order = c(1,0,0))
print(AR_fit)

# Use predict() to make a 1-step forecast
predict_AR <- predict(AR_fit)

# Obtain the 1-step forecast using $pred[1]
predict_AR$pred[1]

# Use predict to make 1-step through 10-step forecasts
predict(AR_fit, n.ahead = 10)

# Run to plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(Nile, xlim = c(1871, 1980))
AR_forecast <- predict(AR_fit, n.ahead = 10)$pred
AR_forecast_se <- predict(AR_fit, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2*AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2*AR_forecast_se, type = "l", col = 2, lty = 2)

